"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[307],{59:(e,t,a)=>{a.d(t,{A:()=>n});const n=a.p+"assets/images/pixcell_overview-22761ba38a029014e04c1d0c27825fae.png"},3272:(e,t,a)=>{a.d(t,{A:()=>n});const n=a.p+"assets/images/1024_variations-09d22e8b72faed33c657594e3f54c611.png"},4504:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>s,metadata:()=>o,toc:()=>c});var n=a(4848),i=a(8453);const s={sidebar_label:"PixCell - Generative foundation model"},r="PixCell: A generative foundation model for digital histopathology images",o={id:"projects/pixcell",title:"PixCell: A generative foundation model for digital histopathology images",description:"Preprint",source:"@site/docs/projects/pixcell.md",sourceDirName:"projects",slug:"/projects/pixcell",permalink:"/docs/projects/pixcell",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{sidebar_label:"PixCell - Generative foundation model"},sidebar:"projectsSidebar",next:{title:"PathAE - Pathology image compression with autoencoders",permalink:"/docs/projects/pathae"}},l={},c=[{value:"Citation",id:"citation",level:3}];function d(e){const t={a:"a",br:"br",code:"code",h1:"h1",h3:"h3",hr:"hr",img:"img",p:"p",pre:"pre",strong:"strong",...(0,i.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(t.h1,{id:"pixcell-a-generative-foundation-model-for-digital-histopathology-images",children:"PixCell: A generative foundation model for digital histopathology images"}),"\n",(0,n.jsx)("div",{class:"container mt-5",children:(0,n.jsx)("div",{class:"card bg-light",children:(0,n.jsxs)("div",{class:"card-body justify-content-center",children:[(0,n.jsx)("h2",{class:"card-title text-center",children:"Preprint"}),(0,n.jsxs)("h3",{class:"authors card-title text-center",children:["Srikar Yellapragada",(0,n.jsx)("sup",{children:"1"}),", Alexandros Graikos",(0,n.jsx)("sup",{children:"1"}),", Zilinghan Li",(0,n.jsx)("sup",{children:"2"}),", Kostas Triaridis",(0,n.jsx)("sup",{children:"1"}),",",(0,n.jsx)(t.br,{}),"\n","Varun Belagali",(0,n.jsx)("sup",{children:"1"}),", Saarthak Kapse",(0,n.jsx)("sup",{children:"1"}),", Tarak Nath Nandi",(0,n.jsx)("sup",{children:"2,3"}),", Ravi K Madduri",(0,n.jsx)("sup",{children:"2,3"}),",",(0,n.jsx)(t.br,{}),"\n","Prateek Prasanna",(0,n.jsx)("sup",{children:"1"}),", Tahsin Kurc",(0,n.jsx)("sup",{children:"1"}),", Rajarsi R. Gupta",(0,n.jsx)("sup",{children:"1"}),", Joel Saltz",(0,n.jsx)("sup",{children:"1"}),", Dimitris Samaras",(0,n.jsx)("sup",{children:"1"})]}),(0,n.jsxs)("h3",{class:"authors card-text text-center",children:[(0,n.jsx)("sup",{children:"1"}),"Stony Brook University, ",(0,n.jsx)("sup",{children:"2"}),"Argonne National Laboratory, ",(0,n.jsx)("sup",{children:"3"}),"University of Chicago"]}),(0,n.jsxs)("div",{class:"d-flex justify-content-center",children:[(0,n.jsxs)("a",{href:"https://arxiv.org/abs/2506.05127",target:"_blank",children:[(0,n.jsx)("button",{class:"paper_button",children:"arXiv"})," "]}),(0,n.jsx)("a",{href:"https://huggingface.co/StonyBrook-CVLab/PixCell-1024",target:"_blank",children:(0,n.jsx)("button",{class:"paper_button",children:"PixCell-1024 \ud83e\udd17"})}),(0,n.jsx)("a",{href:"https://huggingface.co/StonyBrook-CVLab/PixCell-256",target:"_blank",children:(0,n.jsx)("button",{class:"paper_button",children:"PixCell-256 \ud83e\udd17"})})]}),(0,n.jsxs)("div",{class:"d-flex justify-content-center",children:[(0,n.jsx)("a",{href:"https://huggingface.co/StonyBrook-CVLab/PixCell-256-Cell-ControlNet",target:"_blank",children:(0,n.jsx)("button",{class:"paper_button",children:"PixCell-256-Cell-ControlNet \ud83e\udd17"})}),(0,n.jsx)("a",{href:"https://huggingface.co/datasets/StonyBrook-CVLab/Synthetic-SBU-1M",target:"_blank",children:(0,n.jsx)("button",{class:"paper_button",children:"Synthetic-SBU-1M \ud83e\udd17"})})]})]})})}),"\n",(0,n.jsx)("br",{}),"\n",(0,n.jsx)("br",{}),"\n",(0,n.jsxs)(t.p,{children:["We present ",(0,n.jsx)(t.strong,{children:"PixCell"}),", the first generative foundation model for digital histopathology. We progressively train our model to generate from 256x256 to 1024x1024 pixel images conditioned on ",(0,n.jsx)(t.a,{href:"https://huggingface.co/MahmoodLab/UNI2-h",children:"UNI2-h"})," embeddings. PixCell achieves state-of-the-art quality in digital pathology image generation and can be seamlessly used to perform targeted data augmentation and generative downstream tasks."]}),"\n",(0,n.jsx)(t.hr,{}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{alt:"overview",src:a(59).A+"",width:"1493",height:"718"})}),"\n",(0,n.jsxs)(t.p,{children:["We curate a dataset of 30 million patches at 1024x1024 resolution from public and internal digital histopathology datasets, ",(0,n.jsx)(t.strong,{children:"PanCan-30M"}),". We pair each sample in the dataset with its corresponding 16 UNI-2h embeddings to construct the training data. We progressively train a diffusion transformer model, starting from 256x256 resolution and scaling up to 1024x1024. We release the weights for both the 256x256 model (",(0,n.jsx)(t.a,{href:"https://huggingface.co/StonyBrook-CVLab/PixCell-256",children:"PixCell-256"}),") and the 1024x1024 model (",(0,n.jsx)(t.a,{href:"https://huggingface.co/StonyBrook-CVLab/PixCell-1024",children:"PixCell-1024"}),")."]}),"\n",(0,n.jsx)("br",{}),"\n",(0,n.jsx)(t.hr,{}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{alt:"256_variations",src:a(7592).A+"",width:"1086",height:"492"})}),"\n",(0,n.jsx)("br",{}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{alt:"1024_variations",src:a(3272).A+"",width:"1086",height:"387"})}),"\n",(0,n.jsxs)(t.p,{children:["Given a UNI embedding from a reference image, PixCell generates images with similar appearance and content. Using the PixCell-256 model we generate a synthetic variant of our SBU-1M dataset, ",(0,n.jsx)(t.a,{href:"https://huggingface.co/datasets/StonyBrook-CVLab/Synthetic-SBU-1M",children:"Synthetic SBU-1M"}),". We show that we can train an encoder on the synthetic data only without losing any performance on downstream tasks."]}),"\n",(0,n.jsx)("div",{class:"container text-center",children:(0,n.jsx)("img",{src:"/img/pixcell/ssl_results.png",width:"700"})}),"\n",(0,n.jsx)("br",{}),"\n",(0,n.jsx)(t.hr,{}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{alt:"controlnet",src:a(9067).A+"",width:"1405",height:"708"})}),"\n",(0,n.jsxs)(t.p,{children:["Using a pre-trained cell segmentation model, we construct a dataset of 10,000 image-cell mask pairs. We train a cell mask ControlNet (",(0,n.jsx)(t.a,{href:"https://huggingface.co/StonyBrook-CVLab/PixCell-256-Cell-ControlNet",children:"PixCell-256-Cell-ControlNet"}),") on this dataset to guide image generation. Using the UNI embedding from a reference image to control the ",(0,n.jsx)(t.strong,{children:"style"})," and a cell mask to control the ",(0,n.jsx)(t.strong,{children:"layout"}),", we can generate targeted synthetic data using the appearances from the test set and masks from training set to improve the downstream cell segmentation task."]}),"\n",(0,n.jsx)("br",{}),"\n",(0,n.jsx)(t.hr,{}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{alt:"stain_translation",src:a(6226).A+"",width:"1424",height:"861"})}),"\n",(0,n.jsx)(t.p,{children:'Our PixCell models, although never trained explicitly on IHC-stained data, can generalize to IHC images. Using a dataset of "roughly" paired H&E and IHC patches we learn a transformation between H&E UNI embeddings and IHC UNI embeddings. We use this learned transformation to perform stain translation between the two different staining techniques, significantly outperforming previous GAN-based models.'}),"\n",(0,n.jsx)(t.h3,{id:"citation",children:"Citation"}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-bibtex",children:"@article{yellapragada2025pixcell,\n  title={PixCell: A generative foundation model for digital histopathology images},\n  author={Yellapragada, Srikar and Graikos, Alexandros and Li, Zilinghan and Triaridis, Kostas and Belagali, Varun and Kapse, Saarthak and Nandi, Tarak Nath and Madduri, Ravi K and Prasanna, Prateek and Kurc, Tahsin and others},\n  journal={arXiv preprint arXiv:2506.05127},\n  year={2025}\n}\n"})})]})}function h(e={}){const{wrapper:t}={...(0,i.R)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(d,{...e})}):d(e)}},6226:(e,t,a)=>{a.d(t,{A:()=>n});const n=a.p+"assets/images/translation_images-d1c0783f8d380cc21b8ca6514f9bb783.png"},7592:(e,t,a)=>{a.d(t,{A:()=>n});const n=a.p+"assets/images/256_variations-51d30bd3662580bb4be637c2f32050bf.png"},8453:(e,t,a)=>{a.d(t,{R:()=>r,x:()=>o});var n=a(6540);const i={},s=n.createContext(i);function r(e){const t=n.useContext(s);return n.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),n.createElement(s.Provider,{value:t},e.children)}},9067:(e,t,a)=>{a.d(t,{A:()=>n});const n=a.p+"assets/images/controlnet_images-42afc4e5653fc190db94a4cab9501acd.png"}}]);