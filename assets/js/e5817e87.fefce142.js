"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[307],{59:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/pixcell_overview-ad9591148a5694d7f5cdf75ecaa41a2b.png"},4504:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>s,metadata:()=>o,toc:()=>c});var a=n(4848),i=n(8453);const s={sidebar_label:"PixCell - Generative foundation model"},r="PixCell: A generative foundation model for digital histopathology images",o={id:"projects/pixcell",title:"PixCell: A generative foundation model for digital histopathology images",description:"Preprint",source:"@site/docs/projects/pixcell.md",sourceDirName:"projects",slug:"/projects/pixcell",permalink:"/docs/projects/pixcell",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{sidebar_label:"PixCell - Generative foundation model"},sidebar:"projectsSidebar",next:{title:"PathAE - Pathology image compression with autoencoders",permalink:"/docs/projects/pathae"}},l={},c=[{value:"Citation",id:"citation",level:3}];function d(e){const t={a:"a",br:"br",code:"code",h1:"h1",h3:"h3",img:"img",p:"p",pre:"pre",strong:"strong",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.h1,{id:"pixcell-a-generative-foundation-model-for-digital-histopathology-images",children:"PixCell: A generative foundation model for digital histopathology images"}),"\n",(0,a.jsx)("div",{class:"container mt-5",children:(0,a.jsx)("div",{class:"card bg-light",children:(0,a.jsxs)("div",{class:"card-body justify-content-center",children:[(0,a.jsx)("h2",{class:"card-title text-center",children:"Preprint"}),(0,a.jsxs)("h3",{class:"card-title text-center",children:["Srikar Yellapragada",(0,a.jsx)("sup",{children:"1"}),", Alexandros Graikos",(0,a.jsx)("sup",{children:"1"}),", Zilinghan Li",(0,a.jsx)("sup",{children:"2"}),", Kostas Triaridis",(0,a.jsx)("sup",{children:"1"}),",",(0,a.jsx)(t.br,{}),"\n","Varun Belagali",(0,a.jsx)("sup",{children:"1"}),", Saarthak Kapse",(0,a.jsx)("sup",{children:"1"}),", Tarak Nath Nandi",(0,a.jsx)("sup",{children:"2,3"}),", Ravi K Madduri",(0,a.jsx)("sup",{children:"2,3"}),",",(0,a.jsx)(t.br,{}),"\n","Prateek Prasanna",(0,a.jsx)("sup",{children:"1"}),", Tahsin Kurc",(0,a.jsx)("sup",{children:"1"}),", Rajarsi R. Gupta",(0,a.jsx)("sup",{children:"1"}),", Joel Saltz",(0,a.jsx)("sup",{children:"1"}),", Dimitris Samaras",(0,a.jsx)("sup",{children:"1"})]}),(0,a.jsxs)("h3",{class:"card-text text-center",children:[(0,a.jsx)("sup",{children:"1"}),"Stony Brook University, ",(0,a.jsx)("sup",{children:"2"}),"Argonne National Laboratory, ",(0,a.jsx)("sup",{children:"3"}),"University of Chicago"]}),(0,a.jsxs)("div",{class:"d-flex justify-content-center",children:[(0,a.jsxs)("a",{href:"",target:"_blank",children:[(0,a.jsx)("button",{class:"paper_button",children:"arXiv"})," "]}),(0,a.jsx)("a",{href:"https://huggingface.co/StonyBrook-CVLab/PixCell-1024",target:"_blank",children:(0,a.jsx)("button",{class:"paper_button",children:"PixCell-1024 \ud83e\udd17"})}),(0,a.jsx)("a",{href:"https://huggingface.co/StonyBrook-CVLab/PixCell-256",target:"_blank",children:(0,a.jsx)("button",{class:"paper_button",children:"PixCell-256 \ud83e\udd17"})})]}),(0,a.jsxs)("div",{class:"d-flex justify-content-center",children:[(0,a.jsx)("a",{href:"https://huggingface.co/StonyBrook-CVLab/PixCell-256-Cell-ControlNet",target:"_blank",children:(0,a.jsx)("button",{class:"paper_button",children:"PixCell-256-Cell-ControlNet \ud83e\udd17"})}),(0,a.jsx)("a",{href:"https://huggingface.co/StonyBrook-CVLab/Synthetic-SBU-1M",target:"_blank",children:(0,a.jsx)("button",{class:"paper_button",children:"Synthetic-SBU-1M \ud83e\udd17"})})]})]})})}),"\n",(0,a.jsx)("br",{}),"\n",(0,a.jsx)("br",{}),"\n",(0,a.jsxs)(t.p,{children:["We present ",(0,a.jsx)(t.strong,{children:"PixCell"}),", the first generative foundation model for digital histopathology. We progressively train our model to generate from 256x256 to 1024x1024 pixel images conditioned on ",(0,a.jsx)(t.a,{href:"https://huggingface.co/MahmoodLab/UNI2-h",children:"UNI2-h"})," embeddings. PixCell achieves state-of-the-art quality in digital pathology image generation and can be seamlessly used to perform targeted data augmentation and generative downstream tasks."]}),"\n",(0,a.jsx)("br",{}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{alt:"overview",src:n(59).A+"",width:"3404",height:"1465"})}),"\n",(0,a.jsxs)(t.p,{children:["We curate a dataset of 30 million patches at 1024x1024 resolution from public and internal digital histopathology datasets, ",(0,a.jsx)(t.strong,{children:"PanCan-30M"}),". We pair each sample in the dataset with its corresponding 16 UNI-2h embeddings to construct the training data. We progressively train a diffusion transformer model, starting from 256x256 resolution and scaling up to 1024x1024. We release the weights for both the 256x256 model (",(0,a.jsx)(t.a,{href:"https://huggingface.co/StonyBrook-CVLab/PixCell-256",children:"PixCell-256"}),") and the 1024x1024 model (",(0,a.jsx)(t.a,{href:"https://huggingface.co/StonyBrook-CVLab/PixCell-1024",children:"PixCell-1024"}),")."]}),"\n",(0,a.jsx)("br",{}),"\n",(0,a.jsx)("div",{class:"container text-center",children:(0,a.jsx)("img",{src:"/img/pixcell/variations.png",height:"800"})}),"\n",(0,a.jsx)("br",{}),"\n",(0,a.jsxs)(t.p,{children:["Given a UNI embedding from a reference image, PixCell generates images with similar appearance and content. Using the PixCell-256 model we generate a synthetic variant of our SBU-1M dataset, ",(0,a.jsx)(t.a,{href:"https://huggingface.co/datasets/StonyBrook-CVLab/Synthetic-SBU-1M",children:"Synthetic SBU-1M"}),". We show that we can train an encoder on the synthetic data only without losing any performance on downstream tasks."]}),"\n",(0,a.jsx)("div",{class:"container text-center",children:(0,a.jsx)("img",{src:"/img/pixcell/ssl_results.png",width:"700"})}),"\n",(0,a.jsx)("br",{}),"\n",(0,a.jsx)("br",{}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{alt:"controlnet",src:n(9067).A+"",width:"3067",height:"1560"})}),"\n",(0,a.jsxs)(t.p,{children:["Using a pre-trained cell segmentation model, we construct a dataset of 10,000 image-cell mask pairs. We train a cell mask ControlNet (",(0,a.jsx)(t.a,{href:"https://huggingface.co/StonyBrook-CVLab/PixCell-256-Cell-ControlNet",children:"PixCell-256-Cell-ControlNet"}),") on this dataset to guide image generation. Using the UNI embedding from a reference image to control the ",(0,a.jsx)(t.strong,{children:"style"})," and a cell mask to control the ",(0,a.jsx)(t.strong,{children:"layout"}),", we can generate targeted synthetic data using the appearances from the test set and masks from training set to improve the downstream cell segmentation task."]}),"\n",(0,a.jsx)("br",{}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{alt:"stain_translation",src:n(6226).A+"",width:"2685",height:"1671"})}),"\n",(0,a.jsx)(t.p,{children:'Our PixCell models, although never trained explicitly on IHC-stained data, can generalize to IHC images. Using a dataset of "roughly" paired H&E and IHC patches we learn a transformation between H&E UNI embeddings and IHC UNI embeddings. We use this learned transformation to perform stain translation between the two different staining techniques, significantly outperforming previous GAN-based models.'}),"\n",(0,a.jsx)("br",{}),"\n",(0,a.jsx)(t.h3,{id:"citation",children:"Citation"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-bibtex",children:"TBA\n"})})]})}function h(e={}){const{wrapper:t}={...(0,i.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},6226:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/translation_images-742a01a6e7aa35a8769991f8d1f9f5d4.png"},8453:(e,t,n)=>{n.d(t,{R:()=>r,x:()=>o});var a=n(6540);const i={},s=a.createContext(i);function r(e){const t=a.useContext(s);return a.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),a.createElement(s.Provider,{value:t},e.children)}},9067:(e,t,n)=>{n.d(t,{A:()=>a});const a=n.p+"assets/images/controlnet_images-a77278f936e293a97ce2ab6e58ab2dc5.png"}}]);