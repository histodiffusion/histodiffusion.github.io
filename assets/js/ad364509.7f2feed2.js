"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[39],{6948:(e,t,a)=>{a.d(t,{A:()=>i});const i=a.p+"assets/images/method-775a5d041d4626acfed2a8e1ee9c3100.png"},7516:(e,t,a)=>{a.d(t,{A:()=>i});const i=a.p+"assets/images/brca_patches-e4c1783bb685fcda03e1c4c075b8e443.png"},8347:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>s,metadata:()=>r,toc:()=>c});var i=a(4848),n=a(8453);const s={sidebar_label:"ZoomLDM - Multi-scale diffusion model"},o="ZoomLDM: Latent Diffusion Model for multi-scale image generation",r={id:"projects/zoomldm",title:"ZoomLDM: Latent Diffusion Model for multi-scale image generation",description:"CVPR 2025",source:"@site/docs/projects/zoomldm.md",sourceDirName:"projects",slug:"/projects/zoomldm",permalink:"/docs/projects/zoomldm",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{sidebar_label:"ZoomLDM - Multi-scale diffusion model"},sidebar:"projectsSidebar",previous:{title:"PathAE - Pathology image compression with autoencoders",permalink:"/docs/projects/pathae"},next:{title:"Gen-SIS - Generative augmentations for self-supervised learning",permalink:"/docs/projects/gensis"}},l={},c=[{value:"TL;DR",id:"tldr",level:2},{value:"Model",id:"model",level:2},{value:"Generation Results",id:"generation-results",level:2},{value:"Joint Multi-Scale Generation",id:"joint-multi-scale-generation",level:2},{value:"Multi-Scale Features",id:"multi-scale-features",level:2},{value:"Citation",id:"citation",level:2}];function d(e){const t={br:"br",code:"code",h1:"h1",h2:"h2",img:"img",p:"p",pre:"pre",strong:"strong",...(0,n.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.h1,{id:"zoomldm-latent-diffusion-model-for-multi-scale-image-generation",children:"ZoomLDM: Latent Diffusion Model for multi-scale image generation"}),"\n",(0,i.jsx)("div",{class:"container mt-5",children:(0,i.jsx)("div",{class:"card bg-light",children:(0,i.jsxs)("div",{class:"card-body justify-content-center",children:[(0,i.jsx)("h2",{class:"card-title text-center",children:"CVPR 2025"}),(0,i.jsxs)("h3",{class:"card-text text-center",children:["Srikar Yellapragada*, Alexandros Graikos*, Kostas Triaridis,",(0,i.jsx)(t.br,{}),"\n","Prateek Prasanna, Rajarsi Gupta, Joel Saltz, Dimitris Samaras"]}),(0,i.jsx)("h3",{class:"card-text text-center",children:"Stony Brook University"}),(0,i.jsxs)("div",{class:"d-flex justify-content-center",children:[(0,i.jsxs)("a",{href:"https://github.com/cvlab-stonybrook/ZoomLDM/tree/main",target:"_blank",children:[(0,i.jsx)("button",{class:"paper_button",children:"Code"})," "]}),(0,i.jsxs)("a",{href:"https://arxiv.org/abs/2411.16969",target:"_blank",children:[(0,i.jsx)("button",{class:"paper_button",children:"arXiv"})," "]}),(0,i.jsx)("a",{href:"/pages/zoomldm_large_images/large_images.html",target:"_blank",children:(0,i.jsx)("button",{class:"paper_button",children:"Large image viewer"})})]})]})})}),"\n",(0,i.jsx)(t.h2,{id:"tldr",children:"TL;DR"}),"\n",(0,i.jsxs)(t.p,{children:["Using a diffusion model to generate ",(0,i.jsx)(t.strong,{children:"digital pathology"})," and ",(0,i.jsx)(t.strong,{children:"satellite"})," has been impractical due to their ",(0,i.jsx)(t.strong,{children:"gigapixel sizes"}),". Our previous works focused on generating small patches, which was limited to only capturing local context. Here we propose ",(0,i.jsx)(t.strong,{children:"ZoomLDM"}),", a ",(0,i.jsx)(t.strong,{children:"multi-scale latent diffusion model"})," that can synthesize large images in these domains. We introduce a novel magnification-aware conditioning mechanism that utilizes self-supervised learning (SSL) embeddings and allows the diffusion model to synthesize images at different 'zoom' levels. Using the multi-scale nature of ZoomLDM we also propose an algorithm for computationally tractable and globally coherent ",(0,i.jsx)(t.strong,{children:"image synthesis of up to 4096x4096 pixels"}),", achieving unparalleled quality. Finally, we demonstrate that the multi-scale features extracted from ZoomLDM are highly effective in multiple instance learning tasks."]}),"\n",(0,i.jsx)(t.h2,{id:"model",children:"Model"}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"method_figure",src:a(6948).A+"",width:"1903",height:"741"})}),"\n",(0,i.jsx)(t.p,{children:"We first extract 256 \xd7 256 patches from large images at the initial scale (20\xd7 for pathology) and generate SSL embedding matrices using pretrained encoders (UNI). The large image is then progressively downsampled by a factor of 2, with patches at each scale paired with the SSL embeddings of all overlapping initial-scale patches. Then, the SSL embeddings and magnification level are fed to the Summarizer, which projects them into the cross-magnification Latent space. The diffusion model is trained to generate 256 \xd7 256 patches conditioned on the Summarizer\u2019s output."}),"\n",(0,i.jsx)(t.h2,{id:"generation-results",children:"Generation Results"}),"\n",(0,i.jsx)("div",{class:"container text-center",children:(0,i.jsx)("img",{src:"/img/zoomldm/fid.png"})}),"\n",(0,i.jsx)(t.p,{children:"As indicated by the FID, ZoomLDM achieves superior performance across all magnifications compared to SoTA models. We see larger improvements for magnifications below 2.5x where the data scarcity severely impacts a model\u2019s ability to synthesize diverse, high-quality images. By leveraging data and conditioning across all magnifications, we allow the low density data regions to benefit from the insights that the model gains from the entire dataset, improving both model performance and efficiency."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"patches",src:a(7516).A+"",width:"1469",height:"853"})}),"\n",(0,i.jsx)(t.h2,{id:"joint-multi-scale-generation",children:"Joint Multi-Scale Generation"}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"multi_scale_gen",src:a(8458).A+"",width:"2318",height:"674"})}),"\n",(0,i.jsxs)(t.p,{children:["We also introduce a joint sampling process that synthesizes images across two different scales. We jointly generate a 256\xd7256 image at 1.25x and a 4096x4096 image at 20x, using the 1.25x generation to guide the global structure of the 20x image. Without this context each 20x patch is unaware of its surroundings and the generated image wouldn't be globally coherent. With our approach, the generated large 20x image has a realistic global arrangement of cells and tissue. You can view more examples in our ",(0,i.jsx)("a",{href:"/pages/zoomldm_large_images/large_images.html",target:"_blank",children:"large image viewer"}),"."]}),"\n",(0,i.jsx)(t.h2,{id:"multi-scale-features",children:"Multi-Scale Features"}),"\n",(0,i.jsx)("div",{class:"container text-center",children:(0,i.jsx)("img",{src:"/img/zoomldm/mil.png"})}),"\n",(0,i.jsx)(t.p,{children:"We utilize ZoomLDM as a feature extractor and apply an MIL approach for slide-level classification tasks of Breast cancer subtyping and Homologous Recombintion Deficiency (HRD) prediction. The results show that ZoomLDM\u2019s multi-scale features (fusing 20x and 5x) outperform SoTA encoders in both tasks highlighting the effectiveness of ZoomLDM\u2019s cross-magnification latent space in capturing multi-scale dependencies. Even in a single magnification, ZoomLD outperforms all SoTA encoders. We believe that by learning to synthesize images on top of the capabilities of the discriminative SSL encoders we can exceed previous models in representation learning."}),"\n",(0,i.jsx)(t.h2,{id:"citation",children:"Citation"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-bibtex",children:"@article{yellapragada2024zoomldm,\n  title={ZoomLDM: Latent Diffusion Model for multi-scale image generation},\n  author={Yellapragada, Srikar and Graikos, Alexandros and Triaridis, Kostas and Prasanna, Prateek and Gupta, Rajarsi R and Saltz, Joel and Samaras, Dimitris},\n  journal={arXiv preprint arXiv:2411.16969},\n  year={2024}\n}\n"})})]})}function m(e={}){const{wrapper:t}={...(0,n.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,t,a)=>{a.d(t,{R:()=>o,x:()=>r});var i=a(6540);const n={},s=i.createContext(n);function o(e){const t=i.useContext(s);return i.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:o(e.components),i.createElement(s.Provider,{value:t},e.children)}},8458:(e,t,a)=>{a.d(t,{A:()=>i});const i=a.p+"assets/images/multi_scale_examples_brca-188cc23bf7fb3543d028df308c7d4386.png"}}]);