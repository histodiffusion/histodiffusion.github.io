"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[310],{1050:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>s,metadata:()=>o,toc:()=>d});var i=t(4848),a=t(8453);const s={sidebar_label:"Gen-SIS - Generative augmentations for self-supervised learning"},r="Gen-SIS: Generative Self-augmentation Improves Self-supervised Learning",o={id:"projects/gensis",title:"Gen-SIS: Generative Self-augmentation Improves Self-supervised Learning",description:"Preprint",source:"@site/docs/projects/gensis.md",sourceDirName:"projects",slug:"/projects/gensis",permalink:"/docs/projects/gensis",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{sidebar_label:"Gen-SIS - Generative augmentations for self-supervised learning"},sidebar:"projectsSidebar",previous:{title:"ZoomLDM - Multi-scale diffusion model",permalink:"/docs/projects/zoomldm"},next:{title:"\u221e-Brush - Large image synthesis in infinite dimensions",permalink:"/docs/projects/infty_brush"}},l={},d=[{value:"TL;DR",id:"tldr",level:2},{value:"Method",id:"method",level:2},{value:"Training",id:"training",level:2},{value:"Results",id:"results",level:2},{value:"Citation",id:"citation",level:2}];function c(e){const n={annotation:"annotation",br:"br",code:"code",em:"em",h1:"h1",h2:"h2",li:"li",math:"math",mi:"mi",mrow:"mrow",p:"p",pre:"pre",semantics:"semantics",span:"span",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h1,{id:"gen-sis-generative-self-augmentation-improves-self-supervised-learning",children:"Gen-SIS: Generative Self-augmentation Improves Self-supervised Learning"}),"\n",(0,i.jsx)("div",{class:"container mt-5",children:(0,i.jsx)("div",{class:"card bg-light",children:(0,i.jsxs)("div",{class:"card-body justify-content-center",children:[(0,i.jsx)("h2",{class:"card-title text-center",children:"Preprint"}),(0,i.jsxs)("h3",{class:"card-text text-center",children:["Varun Belagali*",(0,i.jsx)("sup",{children:"1"}),", Srikar Yellapragada*",(0,i.jsx)("sup",{children:"1"}),", Alexandros Graikos",(0,i.jsx)("sup",{children:"1"}),", Saarthak Kapse",(0,i.jsx)("sup",{children:"1"}),",",(0,i.jsx)(n.br,{}),"\n","Zilinghan Li",(0,i.jsx)("sup",{children:"2"}),", Tarak Nath Nandi",(0,i.jsx)("sup",{children:"2,3"}),", Ravi K Madduri",(0,i.jsx)("sup",{children:"2,3"}),",",(0,i.jsx)(n.br,{}),"\n","Prateek Prasanna",(0,i.jsx)("sup",{children:"1"}),", Joel Saltz",(0,i.jsx)("sup",{children:"1"}),", Dimitris Samaras",(0,i.jsx)("sup",{children:"1"})]}),(0,i.jsxs)("h3",{class:"card-text text-center",children:[(0,i.jsx)("sup",{children:"1"}),"Stony Brook University, ",(0,i.jsx)("sup",{children:"2"}),"Argonne National Laboratory, ",(0,i.jsx)("sup",{children:"3"}),"University of Chicago"]}),(0,i.jsx)("div",{class:"d-flex justify-content-center",children:(0,i.jsxs)("a",{href:"https://arxiv.org/abs/2412.01672",target:"_blank",children:[(0,i.jsx)("button",{class:"paper_button",children:"arXiv"})," "]})})]})})}),"\n",(0,i.jsx)(n.h2,{id:"tldr",children:"TL;DR"}),"\n",(0,i.jsxs)(n.p,{children:["Self-supervised learning (SSL) learns visual representations by training an image encoder to maximize similarity between features of different views of the same image. While SoTA SSL algorithms rely on hand-crafted augmentations (cropping, color jittering), recently, generative diffusion models have been shown to improve SSL by providing a wider range of ",(0,i.jsx)(n.em,{children:"generative"})," data augmentations ",(0,i.jsx)("a",{href:"https://arxiv.org/abs/2312.17742",children:"[1]"}),". The downside is that this usually requires pre-training the diffusion model on large-scale image-text datasets, which might not be available for many specialized domains like histopathology."]}),"\n",(0,i.jsxs)(n.p,{children:["To solve this, we introduce Gen-SIS, a ",(0,i.jsx)(n.strong,{children:"diffusion-based augmentation technique trained exclusively on unlabeled image data"}),". We train an initial SSL encoder using only hand-crafted augmentations. We then use this encoder to train an embedding-conditioned diffusion model. The embedding-conditioned diffusion model can synthesize diverse views of an image as well as interpolate between images in the encoder latent space. We show that these ",(0,i.jsx)(n.em,{children:"self-augmentations"}),", i.e. generative augmentations based on the vanilla SSL encoder embeddings, facilitate the training of a stronger SSL encoder which we complement with our novel distantanglement pretext task. We showcase Gen-SIS's effectiveness by demonstrating performance improvements in both natural images and digital histopathology."]}),"\n",(0,i.jsx)(n.h2,{id:"method",children:"Method"}),"\n",(0,i.jsx)("div",{class:"container justify-content-center text-center",children:(0,i.jsx)("video",{width:"100%",controls:!0,children:(0,i.jsx)("source",{src:"/video/gensis/method.mp4",type:"video/mp4"})})}),"\n",(0,i.jsx)(n.p,{children:"We propose two types of self-augmentations:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Generative Augmentations: In the generative augmentation setting, a synthetic image is generated using a single real image as the source. This involves first extracting an embedding ",(0,i.jsxs)(n.span,{className:"katex",children:[(0,i.jsx)(n.span,{className:"katex-mathml",children:(0,i.jsx)(n.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(n.semantics,{children:[(0,i.jsx)(n.mrow,{children:(0,i.jsx)(n.mi,{children:"e"})}),(0,i.jsx)(n.annotation,{encoding:"application/x-tex",children:"e"})]})})}),(0,i.jsx)(n.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(n.span,{className:"base",children:[(0,i.jsx)(n.span,{className:"strut",style:{height:"0.4306em"}}),(0,i.jsx)(n.span,{className:"mord mathnormal",children:"e"})]})})]})," from the source image using the conditioning-encoder, and then guiding the image generation process with that embedding to create a synthetic image I",(0,i.jsx)("sub",{children:"s"})," = E-LDM(z,e). Generative augmentations introduce novel variations in the shape, size, and position of objects, as well as changes in the background, while preserving the semantic content of the objects in the image."]}),"\n",(0,i.jsxs)(n.li,{children:["Interpolated Augmentations: An interesting property of diffusion models is their ability to generate an image that partially resembles each source image when conditioned on embeddings interpolated from two sources. We leverage this property to produce an interpolated synthetic image from two real source images, which we use to perform a new pretext task during the SSL training. With embeddings e",(0,i.jsx)("sub",{children:"1"})," and e",(0,i.jsx)("sub",{children:"2"})," representing the two source images (I",(0,i.jsx)("sub",{children:"1"}),", I",(0,i.jsx)("sub",{children:"2"}),"), and an interpolation ratio \u03b1, we compute an interpolated embedding e",(0,i.jsx)("sub",{children:"int"})," using spherical linear interpolation (SLERP), e",(0,i.jsx)("sub",{children:"int"})," = SLERP(e",(0,i.jsx)("sub",{children:"1"}),", e",(0,i.jsx)("sub",{children:"2"}),", \u03b1). We choose SLERP over linear interpolation since high-dimensional vectors are concentrated near the surface of the unit sphere. This interpolated embedding serves as the conditioning to generate the synthetic interpolated image, I",(0,i.jsx)("sub",{children:"int"})," = E-LDM(e",(0,i.jsx)("sub",{children:"int"}),")."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"training",children:"Training"}),"\n",(0,i.jsx)("div",{class:"container justify-content-center text-center",children:(0,i.jsx)("video",{width:"50%",controls:!0,children:(0,i.jsx)("source",{src:"/video/gensis/training.mp4",type:"video/mp4"})})}),"\n",(0,i.jsx)(n.p,{children:"We use DINO as our vanilla SSL method. To integrate generative augmentations into SSL, we use the real image and a corresponding synthetic image as an input pair for the SSL pretext task. We also apply hand-crafted augmentations to both real and synthetic images."}),"\n",(0,i.jsx)(n.p,{children:"Since the interpolated image contains components of both source images, we propose a disentanglement task where the network learns to separate the distinct features of each source image used in the interpolation."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-py",metastring:'title="PyTorch-style pseudocode for the disentanglement pretext task" showLineNumbers',children:"# Parameters:\n# gs, gt: student and teacher networks\n# tps, tpt: student and teacher temperatures\n# c: center\n# alpha: interpolation ratio\n\n# Load an image from the dataset\nfor img_1 in loader:\n    # Read secondary source image -- Another random image from the dataset\n    img_2 = ReadImage(secondary(img_1))\n    # Read interpolated image of primary and secondary source image\n    # Can be generated on-the-fly but we pre-generate to reduce training time\n    img_int = ReadInterpImage(img_1, img_2, alpha)\n\n    # Apply vanilla DINO augmentation to get a view of the primary image\n    img_1_view = vanilla_augment(img_1)\n    # Apply vanilla DINO augmentation to get a view of secondary image\n    img_2_view = vanilla_augment(img_2)\n    # Apply vanilla DINO augmentation to get a view of the interpolated image\n    img_int_view = vanilla_augment(img_int)\n\n    # Get student output for interpolated image \n    stud_int = gs(img_int_view)\n    # Get teacher output for primary and secondary images\n    teach_1 = gt(img_1_view).detach()\n    teach_2 = gt(img_2_view).detach()\n\n    # Student sharpening\n    stud_int = softmax(stud_int / tps, dim=1)\n    # Entanglement of teacher output\n    teach_ent = alpha * teach_1 + (1-alpha) * teach_2\n    # Teacher sharpening and centering\n    teach_ent = softmax((teach_ent - c) / tpt, dim=1)\n\n    # Compute disentanglement loss\n    disentanglement_loss = - (teach_ent * log(stud_int)).sum(dim=1).mean()\n"})}),"\n",(0,i.jsx)(n.h2,{id:"results",children:"Results"}),"\n",(0,i.jsx)(n.p,{children:"For our enhanced SSL training, we improve DINO with the Gen-SIS framework and call\nthe method Gen-DINO. In Gen-DINO, we pre-train the ViT-S/16 model with generative and interpolated augmentations."}),"\n",(0,i.jsx)("div",{class:"container text-center",children:(0,i.jsx)("img",{src:"/img/gensis/results.png"})}),"\n",(0,i.jsx)(n.h2,{id:"citation",children:"Citation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bibtex",children:"@article{belagali2024gen,\n  title={Gen-SIS: Generative Self-augmentation Improves Self-supervised Learning},\n  author={Belagali, Varun and Yellapragada, Srikar and Graikos, Alexandros and Kapse, Saarthak and Li, Zilinghan and Nandi, Tarak Nath and Madduri, Ravi K and Prasanna, Prateek and Saltz, Joel and Samaras, Dimitris},\n  journal={arXiv preprint arXiv:2412.01672},\n  year={2024}\n}\n"})})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>o});var i=t(6540);const a={},s=i.createContext(a);function r(e){const n=i.useContext(s);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);