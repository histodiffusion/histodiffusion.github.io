"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[722],{9672:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>r,default:()=>u,frontMatter:()=>s,metadata:()=>o,toc:()=>l});var i=t(4848),a=t(8453);const s={sidebar_label:"CVPR-24-SSL-guidance",sidebar_position:3},r="Learned representation-guided diffusion models for large-image generation",o={id:"cvpr_24",title:"Learned representation-guided diffusion models for large-image generation",description:"CVPR 2024",source:"@site/docs/cvpr_24.md",sourceDirName:".",slug:"/cvpr_24",permalink:"/docs/cvpr_24",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:3,frontMatter:{sidebar_label:"CVPR-24-SSL-guidance",sidebar_position:3},sidebar:"tutorialSidebar",previous:{title:"WACV-24-PathLDM",permalink:"/docs/wacv_24"}},d={},l=[{value:"Abstract",id:"abstract",level:2}];function c(e){const n={h1:"h1",h2:"h2",p:"p",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h1,{id:"learned-representation-guided-diffusion-models-for-large-image-generation",children:"Learned representation-guided diffusion models for large-image generation"}),"\n",(0,i.jsxs)("div",{className:"infobox",children:[(0,i.jsx)("span",{class:"conference-title",children:"CVPR 2024"}),(0,i.jsx)(n.p,{children:"Alexandros Graikos*, Srikar Yellapragada*, Minh-Quan Le, Saarthak Kapse, Prateek Prasanna, Joel Saltz, Dimitris Samaras"}),(0,i.jsxs)("div",{class:"button-group",children:[(0,i.jsx)("button",{class:"button_class",onClick:()=>window.location.href="https://github.com/cvlab-stonybrook/Large-Image-Diffusion",children:"Code"}),(0,i.jsx)("button",{class:"button_class",onClick:()=>window.location.href="https://arxiv.org/abs/2312.07330",children:"arXiv"})]})]}),"\n",(0,i.jsx)(n.h2,{id:"abstract",children:"Abstract"}),"\n",(0,i.jsx)(n.p,{children:"To synthesize high-fidelity samples, diffusion models typically require auxiliary data to guide the generation process. However, it is impractical to procure the painstaking patch-level annotation effort required in specialized domains like histopathology and satellite imagery; it is often performed by domain experts and involves hundreds of millions of patches. Modern-day self-supervised learning (SSL) representations encode rich semantic and visual information. In this paper, we posit that such representations are expressive enough to act as proxies to fine-grained human labels. We introduce a novel approach that trains diffusion models conditioned on embeddings from SSL. Our diffusion models successfully project these features back to high-quality histopathology and remote sensing images. In addition, we construct larger images by assembling spatially consistent patches inferred from SSL embeddings, preserving long-range dependencies. Augmenting real data by generating variations of real images improves downstream classifier accuracy for patch-level and larger, image-scale classification tasks. Our models are effective even on datasets not encountered during training, demonstrating their robustness and generalizability. Generating images from learned embeddings is agnostic to the source of the embeddings. The SSL embeddings used to generate a large image can either be extracted from a reference image, or sampled from an auxiliary model conditioned on any related modality (e.g. class labels, text, genomic data). As proof of concept, we introduce the text-to-large image synthesis paradigm where we successfully synthesize large pathology and satellite images out of text descriptions."})]})}function u(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>o});var i=t(6540);const a={},s=i.createContext(a);function r(e){const n=i.useContext(s);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);